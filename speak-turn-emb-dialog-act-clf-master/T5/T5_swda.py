# -*- coding: utf-8 -*-
"""T5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dqougd_M4WwIcSRJgBRxlEfqvZRbPRs2
"""

# Step 1: Clone the repo but DON'T go inside it
!git clone https://github.com/zihaohe123/speak-turn-emb-dialog-act-clf.git

# Step 2: Install Hugging Face packages
!pip install -U datasets
!pip install transformers==4.28.1

# Step 3: Confirm you're not in the repo
import os
os.chdir("/content")
!pwd  # should be /content

# Step 4: Now safely import!
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import load_dataset

from google.colab import files
uploaded = files.upload()  # Upload your data.zip file

!unzip -o data.zip
!ls data

# Commented out IPython magic to ensure Python compatibility.
# ‚úÖ Step 1: Move out of the cloned repo folder
# %cd /content

# ‚úÖ Step 2: Confirm you're not in the repo
!pwd  # Should show: /content

# ‚úÖ Step 3: Check for conflicting file
!ls speak-turn-emb-dialog-act-clf | grep datasets.py  # Just to confirm it's the issue

# ‚ùå Delete the wrong datasets.py file from /content
!rm -f /content/datasets.py /content/datasets.pyc

import sys
sys.path = [p for p in sys.path if "speak-turn-emb-dialog-act-clf" not in p]

from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import load_dataset

from google.colab import files
uploaded = files.upload()  # Upload your data.zip

!unzip -o data.zip
!ls data

import pandas as pd
import os

# Redefine datasets and fix formatting
datasets = ['mrda', 'swda', 'dyda']
splits = ['train', 'valid', 'test']

for dataset in datasets:
    os.makedirs(f'data/{dataset}', exist_ok=True)
    for split in splits:
        csv_path = f'data/{dataset}/{split}.csv'
        json_path = f'data/{dataset}/{split}.json'
        if os.path.exists(csv_path):
            df = pd.read_csv(csv_path)
            df.to_json(json_path, orient='records', lines=True)  # ‚úÖ fixed
            print(f"‚úÖ Converted {csv_path} ‚Üí {json_path}")
        else:
            print(f"‚ö†Ô∏è {csv_path} not found")



import pandas as pd
import os

# Define all datasets
datasets = ['mrda', 'swda', 'dyda']
splits = ['train', 'valid', 'test']

for dataset in datasets:
    os.makedirs(f'data/{dataset}', exist_ok=True)
    for split in splits:
        csv_path = f'data/{dataset}/{split}.csv'
        json_path = f'data/{dataset}/{split}.json'
        if os.path.exists(csv_path):
            df = pd.read_csv(csv_path)
            df.to_json(json_path, orient='records', lines=False)
            print(f"Converted {csv_path} ‚Üí {json_path}")
        else:
            print(f"‚ö†Ô∏è {csv_path} not found")



from datasets import load_dataset

import pandas as pd

# Rename and convert val.csv to valid.json
df = pd.read_csv("data/mrda/val.csv")
df.to_json("data/mrda/valid.json", orient='records', lines=True)

print("‚úÖ Converted val.csv ‚Üí valid.json")

from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import load_dataset
import os

EPOCHS = 10
BATCH_SIZE = 4
MAX_INPUT_LEN = 128
MAX_LABEL_LEN = 8
DATASETS = ['mrda', 'swda', 'dyda']

tokenizer = T5Tokenizer.from_pretrained("t5-small")

def preprocess(example):
    input_text = "classify: " + str(example["text"])  # üëà force to string
    label_text = str(example["act"])                  # üëà force to string

    model_input = tokenizer(
        input_text,
        padding="max_length",
        truncation=True,
        max_length=MAX_INPUT_LEN
    )
    label_input = tokenizer(
        label_text,
        padding="max_length",
        truncation=True,
        max_length=MAX_LABEL_LEN
    )
    model_input["labels"] = label_input["input_ids"]
    return model_input


def run_t5(dataset_name):
    print(f"\nüöÄ Running T5 on {dataset_name.upper()}")

    data_files = {
        "train": f"data/{dataset_name}/train.json",
        "validation": f"data/{dataset_name}/valid.json",
        "test": f"data/{dataset_name}/test.json"
    }
    dataset = load_dataset("json", data_files=data_files)
    dataset = dataset.map(preprocess, remove_columns=["text", "act"])


    model = T5ForConditionalGeneration.from_pretrained("t5-small")

    training_args = TrainingArguments(
        output_dir=f"./results_{dataset_name}",
        evaluation_strategy="epoch",
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        num_train_epochs=EPOCHS,
        save_strategy="no",
        logging_dir=f"./logs_{dataset_name}",
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        tokenizer=tokenizer
    )

    trainer.train()
    print(f"\n‚úÖ Evaluation on {dataset_name.upper()} test set:")
    results = trainer.evaluate(dataset["test"])
    print(results)

# Run on MRDA, SwDA, DyDA
for name in DATASETS:
    run_t5(name)

import pandas as pd
import os

# Redefine datasets and fix formatting
datasets = ['mrda', 'swda', 'dyda']
splits = ['train', 'valid', 'test']

for dataset in datasets:
    os.makedirs(f'data/{dataset}', exist_ok=True)
    for split in splits:
        csv_path = f'data/{dataset}/{split}.csv'
        json_path = f'data/{dataset}/{split}.json'
        if os.path.exists(csv_path):
            df = pd.read_csv(csv_path)
            df.to_json(json_path, orient='records', lines=True)  # ‚úÖ fixed
            print(f"‚úÖ Converted {csv_path} ‚Üí {json_path}")
        else:
            print(f"‚ö†Ô∏è {csv_path} not found")

from datasets import load_dataset

import pandas as pd

# Rename and convert val.csv to valid.json
df = pd.read_csv("data/swda/val.csv")
df.to_json("data/swda/valid.json", orient='records', lines=True)

print("‚úÖ Converted val.csv ‚Üí valid.json")

from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import load_dataset
import os

# üîÅ Settings specific for SwDA
EPOCHS = 10
BATCH_SIZE = 4
MAX_INPUT_LEN = 128
MAX_LABEL_LEN = 8
DATASETS = ['swda']  # ‚úÖ Only SwDA

# Load tokenizer
tokenizer = T5Tokenizer.from_pretrained("t5-small")

# üßπ Clean & preprocess
def preprocess(example):
    input_text = "classify: " + str(example["text"])
    label_text = str(example["act"])  # Ensure column is named 'act'

    model_input = tokenizer(
        input_text,
        padding="max_length",
        truncation=True,
        max_length=MAX_INPUT_LEN
    )
    label_input = tokenizer(
        label_text,
        padding="max_length",
        truncation=True,
        max_length=MAX_LABEL_LEN
    )
    model_input["labels"] = label_input["input_ids"]
    return model_input

# üöÄ Train T5
def run_t5(dataset_name):
    print(f"\nüöÄ Running T5 on {dataset_name.upper()}")

    # Load JSON files
    data_files = {
        "train": f"data/{dataset_name}/train.json",
        "validation": f"data/{dataset_name}/valid.json",
        "test": f"data/{dataset_name}/test.json"
    }

    dataset = load_dataset("json", data_files=data_files)
    dataset = dataset.map(preprocess, remove_columns=["text", "act"])  # Assuming act is the label column

    # Load model
    model = T5ForConditionalGeneration.from_pretrained("t5-small")

    # Training settings
    training_args = TrainingArguments(
        output_dir=f"./results_{dataset_name}",
        evaluation_strategy="epoch",
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        num_train_epochs=EPOCHS,
        save_strategy="no",
        logging_dir=f"./logs_{dataset_name}",
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        tokenizer=tokenizer
    )

    trainer.train()

    # Evaluate
    print(f"\n‚úÖ Evaluation on {dataset_name.upper()} test set:")
    results = trainer.evaluate(dataset["test"])
    print(results)

# ‚úÖ Run only on SwDA
for name in DATASETS:
    run_t5(name)