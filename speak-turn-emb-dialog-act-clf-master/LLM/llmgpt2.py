# -*- coding: utf-8 -*-
"""LLMGPT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RDkKnjPmxXkzzRGTQgN59I5_UzyLZd5i
"""

!pip install transformers datasets

from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name).to(device)

# GPT-2 doesn't have a pad token by default
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.eos_token_id

def create_prompt(example_text, few_shot_examples):
    prompt = "You are a dialogue act classifier. Classify the dialogue act of the given sentence.\n\n"
    for ex_text, ex_label in few_shot_examples:
        prompt += f"Sentence: {ex_text}\nLabel: {ex_label}\n\n"
    prompt += f"Sentence: {example_text}\nLabel:"
    return prompt

few_shot_mrda = [
    ("I think we should discuss the budget.", "Statement"),
    ("Okay, let's start with the first item.", "Backchannel"),
    ("Can you explain that again?", "Question"),
    ("Yes, I understand.", "Acknowledgment"),
    ("Thank you.", "Appreciation"),
]

# Inference
def classify(text, few_shot_examples):
    prompt = create_prompt(text, few_shot_examples)
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=10)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded.split("Label:")[-1].strip().split("\n")[0]

# Example sentence
test_sentence = "Could we go back to the earlier point?"
print("Predicted Label:", classify(test_sentence, few_shot_mrda))

few_shot_swda = [
    ("How are you doing today?", "Greeting"),
    ("I'm good, thank you!", "Statement"),
    ("Oh, really?", "Backchannel"),
    ("Do you know what time it is?", "Yes-No Question"),
    ("I don't think that's right.", "Disagreement"),
]
# Inference
def classify(text, few_shot_examples):
    prompt = create_prompt(text, few_shot_examples)
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=10)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded.split("Label:")[-1].strip().split("\n")[0]

# Example sentence
test_sentence = "Could we go back to the earlier point?"
print("Predicted Label:", classify(test_sentence, few_shot_swda))

few_shot_dyda = [
    ("Let's start the presentation.", "Inform"),
    ("What is the purpose of this study?", "Question"),
    ("That's a good idea.", "Agreement"),
    ("Please proceed to the next slide.", "Instruction"),
    ("I'm not sure.", "Uncertainty"),
]
# Inference
def classify(text, few_shot_examples):
    prompt = create_prompt(text, few_shot_examples)
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=10)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded.split("Label:")[-1].strip().split("\n")[0]

# Example sentence
test_sentence = "Could we go back to the earlier point?"
print("Predicted Label:", classify(test_sentence, few_shot_dyda))

test_samples = ["Could you clarify that?", "Alright, moving on.", "Why do you think that happened?"]

for sentence in test_samples:
    print(f"Sentence: {sentence}")
    print("Predicted:", classify(sentence, few_shot_mrda))
    print("-----")

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/zihaohe123/speak-turn-emb-dialog-act-clf.git
# %cd speak-turn-emb-dialog-act-clf

!unzip data.zip

import pandas as pd
mrda_test = pd.read_csv("data/mrda/test.csv")
mrda_test.head()

print(mrda_test.columns)

few_shot_mrda = list(mrda_test[['text', 'act']].iloc[:10].itertuples(index=False, name=None))

print(few_shot_mrda)

from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2").to(device)

# Set pad token
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.eos_token_id

def create_prompt(example_text, few_shot_examples):
    prompt = "You are a dialogue act classifier. Classify the dialogue act of the given sentence.\n\n"
    for ex_text, ex_label in few_shot_examples:
        prompt += f"Sentence: {ex_text}\nLabel: {ex_label}\n\n"
    prompt += f"Sentence: {example_text}\nLabel:"
    return prompt

def classify(text, few_shot_examples):
    prompt = create_prompt(text, few_shot_examples)
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=10)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded.split("Label:")[-1].strip().split("\n")[0]

label_map = {
    0: "Statement",
    1: "Backchannel",
    2: "Question",
    3: "Agreement",
    4: "Disruption"
}

print(mrda_test.columns)

# Correct label map for MRDA
label_map = {
    0: "Statement",
    1: "Backchannel",
    2: "Question",
    3: "Agreement",
    4: "Disruption"
}

# Use actual column names
test_sentence = mrda_test['text'].iloc[20]
true_label = label_map[mrda_test['act'].iloc[20]]

pred = classify(test_sentence, few_shot_mrda)

print("Sentence:", test_sentence)
print("True Label:", true_label)
print("Predicted Label:", pred)

swda_test = pd.read_csv("data/swda/test.csv")
swda_test.columns

swda_label_map = {
    0: "Statement",
    1: "Backchannel",
    2: "Question",
    3: "Agreement",
    4: "Disagreement",
    5: "Clarification",
    6: "Acknowledge",
    7: "Appreciation",
    8: "Other"
}

swda_test['act'].unique()  # Check available label IDs

unique_labels = sorted(swda_test['act'].unique())
swda_label_map = {i: f"Class_{i}" for i in unique_labels}

print(dict(list(swda_label_map.items())[:10]))

few_shot_swda = [
    (text, swda_label_map[label])
    for text, label in zip(swda_test['text'][:10], swda_test['act'][:10])
]

test_sentence_swda = swda_test['text'].iloc[10]
true_label_swda = swda_label_map[swda_test['act'].iloc[10]]

pred_swda = classify(test_sentence_swda, few_shot_swda)

print("Sentence:", test_sentence_swda)
print("True Label:", true_label_swda)
print("Predicted Label:", pred_swda)

dyda_test = pd.read_csv("data/dyda/test.csv")
print(dyda_test.columns)

dyda_test['act'].unique()

dyda_label_map = {
    0: "Statement",
    1: "Question",
    2: "Command",
    3: "Backchannel"
}

few_shot_dyda = [
    (text, dyda_label_map[label])
    for text, label in zip(dyda_test['text'][:10], dyda_test['act'][:10])
]

test_sentence_dyda = dyda_test['text'].iloc[10]
true_label_dyda = dyda_label_map[dyda_test['act'].iloc[10]]

pred_dyda = classify(test_sentence_dyda, few_shot_dyda)

print("Sentence:", test_sentence_dyda)
print("True Label:", true_label_dyda)
print("Predicted Label:", pred_dyda)

def evaluate_gpt2_accuracy(test_df, few_shot_examples, label_map, text_col="text", label_col="act", num_samples=100):
    correct = 0
    total = min(num_samples, len(test_df))

    for i in range(total):
        sentence = test_df[text_col].iloc[i]
        true_label_id = test_df[label_col].iloc[i]
        true_label = label_map[true_label_id]

        pred_label = classify(sentence, few_shot_examples)

        if pred_label.strip().lower() == true_label.strip().lower():
            correct += 1

    accuracy = correct / total
    print(f"Accuracy over {total} samples: {accuracy:.2f}")
    return accuracy

evaluate_gpt2_accuracy(mrda_test, few_shot_mrda, label_map)

evaluate_gpt2_accuracy(swda_test, few_shot_swda, swda_label_map)

evaluate_gpt2_accuracy(dyda_test, few_shot_dyda, dyda_label_map)