{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nvdUSX2Az8d",
        "outputId": "231f670b-d85f-4ce5-f601-bd6165501f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'speak-turn-emb-dialog-act-clf'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 55 (delta 27), reused 37 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (55/55), 7.30 MiB | 30.76 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/zihaohe123/speak-turn-emb-dialog-act-clf.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjvdDD9UGuAU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cScPq3XNBMgd",
        "outputId": "aa0fc2ca-20d8-4f96-ad69-53f80fdcb5e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/speak-turn-emb-dialog-act-clf\n"
          ]
        }
      ],
      "source": [
        "%cd speak-turn-emb-dialog-act-clf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DFpbBM2OB-Lx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoJ1YVfxBiDZ",
        "outputId": "a8790a10-57f7-4b47-b320-c59570164d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/data/\n",
            "  inflating: data/__MACOSX/._data    \n",
            "   creating: data/data/mrda/\n",
            "  inflating: data/__MACOSX/data/._mrda  \n",
            "   creating: data/data/swda/\n",
            "  inflating: data/__MACOSX/data/._swda  \n",
            "   creating: data/data/dyda/\n",
            "  inflating: data/__MACOSX/data/._dyda  \n",
            "  inflating: data/data/mrda/train.csv  \n",
            "  inflating: data/__MACOSX/data/mrda/._train.csv  \n",
            "  inflating: data/data/mrda/val.csv  \n",
            "  inflating: data/__MACOSX/data/mrda/._val.csv  \n",
            "  inflating: data/data/mrda/test.csv  \n",
            "  inflating: data/__MACOSX/data/mrda/._test.csv  \n",
            "  inflating: data/data/swda/test.csv  \n",
            "  inflating: data/__MACOSX/data/swda/._test.csv  \n",
            "  inflating: data/data/swda/train.csv  \n",
            "  inflating: data/__MACOSX/data/swda/._train.csv  \n",
            "  inflating: data/data/swda/val.csv  \n",
            "  inflating: data/__MACOSX/data/swda/._val.csv  \n",
            "  inflating: data/data/dyda/val.csv  \n",
            "  inflating: data/__MACOSX/data/dyda/._val.csv  \n",
            "  inflating: data/data/dyda/test.csv  \n",
            "  inflating: data/__MACOSX/data/dyda/._test.csv  \n",
            "  inflating: data/data/dyda/train.csv  \n",
            "  inflating: data/__MACOSX/data/dyda/._train.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip -d data/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVw8z_iB_1C",
        "outputId": "3fb3a155-4747-44dd-a476-7337d42ce8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09kX7Ur1CQ_e"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOQ49OtnDjFc",
        "outputId": "2a2aca18-1bf8-4676-fd1a-fd3fd7246b86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "  Length      Date    Time    Name\n",
            "---------  ---------- -----   ----\n",
            "        0  2022-01-12 06:13   data/\n",
            "      470  2022-01-12 06:13   __MACOSX/._data\n",
            "        0  2022-01-12 06:13   data/mrda/\n",
            "      470  2022-01-12 06:13   __MACOSX/data/._mrda\n",
            "        0  2022-01-12 06:13   data/swda/\n",
            "      470  2022-01-12 06:13   __MACOSX/data/._swda\n",
            "        0  2022-01-12 06:13   data/dyda/\n",
            "      470  2022-01-12 06:13   __MACOSX/data/._dyda\n",
            "  3852213  2022-01-12 06:12   data/mrda/train.csv\n",
            "      460  2022-01-12 06:13   __MACOSX/data/mrda/._train.csv\n",
            "   784828  2022-01-12 06:12   data/mrda/val.csv\n",
            "      460  2022-01-12 06:13   __MACOSX/data/mrda/._val.csv\n",
            "   731546  2022-01-12 06:12   data/mrda/test.csv\n",
            "      460  2022-01-12 06:13   __MACOSX/data/mrda/._test.csv\n",
            "   270648  2022-01-12 06:13   data/swda/test.csv\n",
            "      460  2022-01-12 06:13   __MACOSX/data/swda/._test.csv\n",
            " 11831172  2022-01-12 06:13   data/swda/train.csv\n",
            "      460  2022-01-12 06:13   __MACOSX/data/swda/._train.csv\n",
            "  1223895  2022-01-12 06:13   data/swda/val.csv\n",
            "      460  2022-01-12 06:13   __MACOSX/data/swda/._val.csv\n",
            "   643384  2022-01-12 06:11   data/dyda/val.csv\n",
            "      460  2022-01-12 06:13   __MACOSX/data/dyda/._val.csv\n",
            "   625850  2022-01-12 06:12   data/dyda/test.csv\n",
            "      460  2022-01-12 06:13   __MACOSX/data/dyda/._test.csv\n",
            "  7058196  2022-01-12 06:12   data/dyda/train.csv\n",
            "      460  2022-01-12 06:13   __MACOSX/data/dyda/._train.csv\n",
            "---------                     -------\n",
            " 27027752                     26 files\n"
          ]
        }
      ],
      "source": [
        "!unzip -l data.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-EFZqFWpFpEj"
      },
      "outputs": [],
      "source": [
        "!rm -r data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LPXsI2bFyQ0",
        "outputId": "b1cde5fa-0b12-46b6-c967-bf1d89a73402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: ./data/\n",
            "  inflating: ./__MACOSX/._data       \n",
            "   creating: ./data/mrda/\n",
            "  inflating: ./__MACOSX/data/._mrda  \n",
            "   creating: ./data/swda/\n",
            "  inflating: ./__MACOSX/data/._swda  \n",
            "   creating: ./data/dyda/\n",
            "  inflating: ./__MACOSX/data/._dyda  \n",
            "  inflating: ./data/mrda/train.csv   \n",
            "  inflating: ./__MACOSX/data/mrda/._train.csv  \n",
            "  inflating: ./data/mrda/val.csv     \n",
            "  inflating: ./__MACOSX/data/mrda/._val.csv  \n",
            "  inflating: ./data/mrda/test.csv    \n",
            "  inflating: ./__MACOSX/data/mrda/._test.csv  \n",
            "  inflating: ./data/swda/test.csv    \n",
            "  inflating: ./__MACOSX/data/swda/._test.csv  \n",
            "  inflating: ./data/swda/train.csv   \n",
            "  inflating: ./__MACOSX/data/swda/._train.csv  \n",
            "  inflating: ./data/swda/val.csv     \n",
            "  inflating: ./__MACOSX/data/swda/._val.csv  \n",
            "  inflating: ./data/dyda/val.csv     \n",
            "  inflating: ./__MACOSX/data/dyda/._val.csv  \n",
            "  inflating: ./data/dyda/test.csv    \n",
            "  inflating: ./__MACOSX/data/dyda/._test.csv  \n",
            "  inflating: ./data/dyda/train.csv   \n",
            "  inflating: ./__MACOSX/data/dyda/._train.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip -d ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5dLG-U1F7mb",
        "outputId": "d03890ef-4275-427a-de16-4f070351f38c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test.csv  train.csv  val.csv\n"
          ]
        }
      ],
      "source": [
        "!ls data/mrda/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdy5gikJGv6c",
        "outputId": "3e443dfd-36f2-4233-963a-e537dd3b472c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.51.3\n",
            "Uninstalling transformers-4.51.3:\n",
            "  Successfully uninstalled transformers-4.51.3\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMmJq4PtHOt_",
        "outputId": "51739eee-70c6-474e-c359-e6640bc67994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.12.5\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.12.5) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.12.5) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.12.5) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.12.5) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.12.5) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.12.5) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.12.5) (2.32.3)\n",
            "Collecting sacremoses (from transformers==4.12.5)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.12.5)\n",
            "  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.12.5) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.12.5) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.12.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.12.5) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.12.5) (2025.4.26)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses->transformers==4.12.5) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses->transformers==4.12.5) (1.4.2)\n",
            "Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.12.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNbBjSBsHYW2",
        "outputId": "881f9a4b-c96a-482b-c1c0-b37a571c23df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: tokenizers 0.21.1\n",
            "Uninstalling tokenizers-0.21.1:\n",
            "  Successfully uninstalled tokenizers-0.21.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y transformers tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9amhLS-FHaRt",
        "outputId": "a1ecf3b9-636f-465d-b4cf-672e1d220fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.28.1\n",
            "  Using cached transformers-4.28.1-py3-none-any.whl.metadata (109 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1)\n",
            "  Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (2025.4.26)\n",
            "Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.28.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7TVHSRVHkHf",
        "outputId": "4ca5cbf8-9ce1-4d53-a99f-9fb1f313b1bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.28.1\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl.metadata (109 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/110.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.0/110.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1)\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (2025.4.26)\n",
            "Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.28.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_of6RXsHvnh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6zOMqDJErPEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec87bb0-5851-403e-c9e7-85ee4b4e21ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python -u engine.py --corpus=mrda --mode=train --gpu=0,1 --batch_size=4 --batch_size_val=2 --epochs=100 --lr=0.0001 --nlayer=1 --chunk_size=350 --dropout=0.5 --nfinetune=1 --speaker_info=none --topic_info=none --nclass=5 --emb_batch=256\n",
            "Namespace(corpus='mrda', mode='train', nclass=5, batch_size=4, batch_size_val=2, emb_batch=256, epochs=100, gpu='0,1', lr=0.0001, nlayer=1, chunk_size=350, dropout=0.5, speaker_info='none', topic_info='none', nfinetune=1, seed=0)\n",
            "Tokenizing train....\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 119kB/s]\n",
            "config.json: 100% 481/481 [00:00<00:00, 3.74MB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 14.7MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 6.58MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 12.9MB/s]\n",
            "Done\n",
            "Tokenizing val....\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Done\n",
            "Tokenizing test....\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Done\n",
            "Done\n",
            "\n",
            "Let's use 1 GPUs!\n",
            "Initializing model....\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 499M/499M [00:01<00:00, 257MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\n",
            "2025-05-07 16:30:07.545396: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746635407.563652    1512 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746635407.568978    1512 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "********************Epoch: 1********************\n",
            "Batch: 1/60\tloss: 1.604\tloss_act:1.604\n",
            "Batch: 4/60\tloss: 1.324\tloss_act:1.324\n",
            "Batch: 7/60\tloss: 1.223\tloss_act:1.223\n",
            "Batch: 10/60\tloss: 1.332\tloss_act:1.332\n",
            "Batch: 13/60\tloss: 1.168\tloss_act:1.168\n",
            "Batch: 16/60\tloss: 1.134\tloss_act:1.134\n",
            "Batch: 19/60\tloss: 1.201\tloss_act:1.201\n",
            "Batch: 22/60\tloss: 1.142\tloss_act:1.142\n",
            "Batch: 25/60\tloss: 1.107\tloss_act:1.107\n",
            "Batch: 28/60\tloss: 1.012\tloss_act:1.012\n",
            "Batch: 31/60\tloss: 0.949\tloss_act:0.949\n",
            "Batch: 34/60\tloss: 0.972\tloss_act:0.972\n",
            "Batch: 37/60\tloss: 0.937\tloss_act:0.937\n",
            "Batch: 40/60\tloss: 0.858\tloss_act:0.858\n",
            "Batch: 43/60\tloss: 0.842\tloss_act:0.842\n",
            "Batch: 46/60\tloss: 0.751\tloss_act:0.751\n",
            "Batch: 49/60\tloss: 0.687\tloss_act:0.687\n",
            "Batch: 52/60\tloss: 0.631\tloss_act:0.631\n",
            "Batch: 55/60\tloss: 0.540\tloss_act:0.540\n",
            "Batch: 58/60\tloss: 0.590\tloss_act:0.590\n",
            "Batch: 60/60\tloss: 0.542\tloss_act:0.542\n",
            "Epoch 1\tTrain Loss: 0.963\tVal Acc: 0.836\tTest Acc: 0.855\n",
            "Best Epoch: 1\tBest Epoch Val Acc: 0.836\tBest Epoch Test Acc: 0.855, Best Test Acc: 0.855\n",
            "\n",
            "********************Epoch: 2********************\n",
            "Batch: 1/60\tloss: 0.504\tloss_act:0.504\n",
            "Batch: 4/60\tloss: 0.368\tloss_act:0.368\n",
            "Batch: 7/60\tloss: 0.328\tloss_act:0.328\n",
            "Batch: 10/60\tloss: 0.417\tloss_act:0.417\n",
            "Batch: 13/60\tloss: 0.352\tloss_act:0.352\n",
            "Batch: 16/60\tloss: 0.375\tloss_act:0.375\n",
            "Batch: 19/60\tloss: 0.343\tloss_act:0.343\n",
            "Batch: 22/60\tloss: 0.312\tloss_act:0.312\n",
            "Batch: 25/60\tloss: 0.348\tloss_act:0.348\n",
            "Batch: 28/60\tloss: 0.365\tloss_act:0.365\n",
            "Batch: 31/60\tloss: 0.373\tloss_act:0.373\n",
            "Batch: 34/60\tloss: 0.302\tloss_act:0.302\n",
            "Batch: 37/60\tloss: 0.370\tloss_act:0.370\n",
            "Batch: 40/60\tloss: 0.300\tloss_act:0.300\n",
            "Batch: 43/60\tloss: 0.393\tloss_act:0.393\n",
            "Batch: 46/60\tloss: 0.391\tloss_act:0.391\n",
            "Batch: 49/60\tloss: 0.369\tloss_act:0.369\n",
            "Batch: 52/60\tloss: 0.335\tloss_act:0.335\n",
            "Batch: 55/60\tloss: 0.290\tloss_act:0.290\n",
            "Batch: 58/60\tloss: 0.377\tloss_act:0.377\n",
            "Batch: 60/60\tloss: 0.314\tloss_act:0.314\n",
            "Epoch 2\tTrain Loss: 0.358\tVal Acc: 0.883\tTest Acc: 0.895\n",
            "Best Epoch: 2\tBest Epoch Val Acc: 0.883\tBest Epoch Test Acc: 0.895, Best Test Acc: 0.895\n",
            "\n",
            "********************Epoch: 3********************\n",
            "Batch: 1/60\tloss: 0.308\tloss_act:0.308\n",
            "Batch: 4/60\tloss: 0.274\tloss_act:0.274\n",
            "Batch: 7/60\tloss: 0.286\tloss_act:0.286\n",
            "Batch: 10/60\tloss: 0.317\tloss_act:0.317\n",
            "Batch: 13/60\tloss: 0.292\tloss_act:0.292\n",
            "Batch: 16/60\tloss: 0.318\tloss_act:0.318\n",
            "Batch: 19/60\tloss: 0.292\tloss_act:0.292\n",
            "Batch: 22/60\tloss: 0.337\tloss_act:0.337\n",
            "Batch: 25/60\tloss: 0.216\tloss_act:0.216\n",
            "Batch: 28/60\tloss: 0.324\tloss_act:0.324\n",
            "Batch: 31/60\tloss: 0.253\tloss_act:0.253\n",
            "Batch: 34/60\tloss: 0.274\tloss_act:0.274\n",
            "Batch: 37/60\tloss: 0.250\tloss_act:0.250\n",
            "Batch: 40/60\tloss: 0.286\tloss_act:0.286\n",
            "Batch: 43/60\tloss: 0.340\tloss_act:0.340\n",
            "Batch: 46/60\tloss: 0.314\tloss_act:0.314\n",
            "Batch: 49/60\tloss: 0.254\tloss_act:0.254\n",
            "Batch: 52/60\tloss: 0.299\tloss_act:0.299\n",
            "Batch: 55/60\tloss: 0.329\tloss_act:0.329\n",
            "Batch: 58/60\tloss: 0.276\tloss_act:0.276\n",
            "Batch: 60/60\tloss: 0.298\tloss_act:0.298\n",
            "Epoch 3\tTrain Loss: 0.301\tVal Acc: 0.883\tTest Acc: 0.899\n",
            "Best Epoch: 2\tBest Epoch Val Acc: 0.883\tBest Epoch Test Acc: 0.895, Best Test Acc: 0.899\n",
            "\n",
            "********************Epoch: 4********************\n",
            "Batch: 1/60\tloss: 0.256\tloss_act:0.256\n",
            "Batch: 4/60\tloss: 0.290\tloss_act:0.290\n",
            "Batch: 7/60\tloss: 0.254\tloss_act:0.254\n",
            "Batch: 10/60\tloss: 0.268\tloss_act:0.268\n",
            "Batch: 13/60\tloss: 0.250\tloss_act:0.250\n",
            "Batch: 16/60\tloss: 0.322\tloss_act:0.322\n",
            "Batch: 19/60\tloss: 0.312\tloss_act:0.312\n",
            "Batch: 22/60\tloss: 0.293\tloss_act:0.293\n",
            "Batch: 25/60\tloss: 0.259\tloss_act:0.259\n",
            "Batch: 28/60\tloss: 0.291\tloss_act:0.291\n",
            "Batch: 31/60\tloss: 0.236\tloss_act:0.236\n",
            "Batch: 34/60\tloss: 0.285\tloss_act:0.285\n",
            "Batch: 37/60\tloss: 0.274\tloss_act:0.274\n",
            "Batch: 40/60\tloss: 0.247\tloss_act:0.247\n",
            "Batch: 43/60\tloss: 0.319\tloss_act:0.319\n",
            "Batch: 46/60\tloss: 0.307\tloss_act:0.307\n",
            "Batch: 49/60\tloss: 0.266\tloss_act:0.266\n",
            "Batch: 52/60\tloss: 0.335\tloss_act:0.335\n",
            "Batch: 55/60\tloss: 0.321\tloss_act:0.321\n",
            "Batch: 58/60\tloss: 0.309\tloss_act:0.309\n",
            "Batch: 60/60\tloss: 0.284\tloss_act:0.284\n",
            "Epoch 4\tTrain Loss: 0.284\tVal Acc: 0.883\tTest Acc: 0.900\n",
            "Best Epoch: 4\tBest Epoch Val Acc: 0.883\tBest Epoch Test Acc: 0.900, Best Test Acc: 0.900\n",
            "\n",
            "********************Epoch: 5********************\n",
            "Batch: 1/60\tloss: 0.339\tloss_act:0.339\n",
            "Batch: 4/60\tloss: 0.182\tloss_act:0.182\n",
            "Batch: 7/60\tloss: 0.253\tloss_act:0.253\n",
            "Batch: 10/60\tloss: 0.281\tloss_act:0.281\n",
            "Batch: 13/60\tloss: 0.330\tloss_act:0.330\n",
            "Batch: 16/60\tloss: 0.262\tloss_act:0.262\n",
            "Batch: 19/60\tloss: 0.286\tloss_act:0.286\n",
            "Batch: 22/60\tloss: 0.347\tloss_act:0.347\n",
            "Batch: 25/60\tloss: 0.293\tloss_act:0.293\n",
            "Batch: 28/60\tloss: 0.254\tloss_act:0.254\n",
            "Batch: 31/60\tloss: 0.272\tloss_act:0.272\n",
            "Batch: 34/60\tloss: 0.277\tloss_act:0.277\n",
            "Batch: 37/60\tloss: 0.327\tloss_act:0.327\n",
            "Batch: 40/60\tloss: 0.297\tloss_act:0.297\n",
            "Batch: 43/60\tloss: 0.258\tloss_act:0.258\n",
            "Batch: 46/60\tloss: 0.275\tloss_act:0.275\n",
            "Batch: 49/60\tloss: 0.244\tloss_act:0.244\n",
            "Batch: 52/60\tloss: 0.238\tloss_act:0.238\n",
            "Batch: 55/60\tloss: 0.290\tloss_act:0.290\n",
            "Batch: 58/60\tloss: 0.273\tloss_act:0.273\n",
            "Batch: 60/60\tloss: 0.305\tloss_act:0.305\n",
            "Epoch 5\tTrain Loss: 0.277\tVal Acc: 0.884\tTest Acc: 0.900\n",
            "Best Epoch: 5\tBest Epoch Val Acc: 0.884\tBest Epoch Test Acc: 0.900, Best Test Acc: 0.900\n",
            "\n",
            "********************Epoch: 6********************\n",
            "Batch: 1/60\tloss: 0.215\tloss_act:0.215\n",
            "Batch: 4/60\tloss: 0.305\tloss_act:0.305\n",
            "Batch: 7/60\tloss: 0.239\tloss_act:0.239\n",
            "Batch: 10/60\tloss: 0.298\tloss_act:0.298\n",
            "Batch: 13/60\tloss: 0.288\tloss_act:0.288\n",
            "Batch: 16/60\tloss: 0.279\tloss_act:0.279\n",
            "Batch: 19/60\tloss: 0.276\tloss_act:0.276\n",
            "Batch: 22/60\tloss: 0.310\tloss_act:0.310\n",
            "Batch: 25/60\tloss: 0.291\tloss_act:0.291\n",
            "Batch: 28/60\tloss: 0.295\tloss_act:0.295\n",
            "Batch: 31/60\tloss: 0.275\tloss_act:0.275\n",
            "Batch: 34/60\tloss: 0.281\tloss_act:0.281\n",
            "Batch: 37/60\tloss: 0.257\tloss_act:0.257\n",
            "Batch: 40/60\tloss: 0.236\tloss_act:0.236\n",
            "Batch: 43/60\tloss: 0.278\tloss_act:0.278\n",
            "Batch: 46/60\tloss: 0.298\tloss_act:0.298\n",
            "Batch: 49/60\tloss: 0.341\tloss_act:0.341\n",
            "Batch: 52/60\tloss: 0.241\tloss_act:0.241\n",
            "Batch: 55/60\tloss: 0.249\tloss_act:0.249\n",
            "Batch: 58/60\tloss: 0.252\tloss_act:0.252\n",
            "Batch: 60/60\tloss: 0.274\tloss_act:0.274\n",
            "Epoch 6\tTrain Loss: 0.270\tVal Acc: 0.885\tTest Acc: 0.903\n",
            "Best Epoch: 6\tBest Epoch Val Acc: 0.885\tBest Epoch Test Acc: 0.903, Best Test Acc: 0.903\n",
            "\n",
            "********************Epoch: 7********************\n",
            "Batch: 1/60\tloss: 0.282\tloss_act:0.282\n",
            "Batch: 4/60\tloss: 0.278\tloss_act:0.278\n",
            "Batch: 7/60\tloss: 0.195\tloss_act:0.195\n",
            "Batch: 10/60\tloss: 0.319\tloss_act:0.319\n",
            "Batch: 13/60\tloss: 0.282\tloss_act:0.282\n",
            "Batch: 16/60\tloss: 0.269\tloss_act:0.269\n",
            "Batch: 19/60\tloss: 0.306\tloss_act:0.306\n",
            "Batch: 22/60\tloss: 0.258\tloss_act:0.258\n",
            "Batch: 25/60\tloss: 0.286\tloss_act:0.286\n",
            "Batch: 28/60\tloss: 0.329\tloss_act:0.329\n",
            "Batch: 31/60\tloss: 0.266\tloss_act:0.266\n",
            "Batch: 34/60\tloss: 0.216\tloss_act:0.216\n",
            "Batch: 37/60\tloss: 0.222\tloss_act:0.222\n",
            "Batch: 40/60\tloss: 0.261\tloss_act:0.261\n",
            "Batch: 43/60\tloss: 0.269\tloss_act:0.269\n",
            "Batch: 46/60\tloss: 0.280\tloss_act:0.280\n",
            "Batch: 49/60\tloss: 0.252\tloss_act:0.252\n",
            "Batch: 52/60\tloss: 0.278\tloss_act:0.278\n",
            "Batch: 55/60\tloss: 0.244\tloss_act:0.244\n",
            "Batch: 58/60\tloss: 0.274\tloss_act:0.274\n",
            "Batch: 60/60\tloss: 0.252\tloss_act:0.252\n",
            "Epoch 7\tTrain Loss: 0.265\tVal Acc: 0.884\tTest Acc: 0.900\n",
            "Best Epoch: 6\tBest Epoch Val Acc: 0.885\tBest Epoch Test Acc: 0.903, Best Test Acc: 0.903\n",
            "\n",
            "********************Epoch: 8********************\n",
            "Batch: 1/60\tloss: 0.272\tloss_act:0.272\n",
            "Batch: 4/60\tloss: 0.249\tloss_act:0.249\n",
            "Batch: 7/60\tloss: 0.269\tloss_act:0.269\n",
            "Batch: 10/60\tloss: 0.272\tloss_act:0.272\n",
            "Batch: 13/60\tloss: 0.261\tloss_act:0.261\n",
            "Batch: 16/60\tloss: 0.303\tloss_act:0.303\n",
            "Batch: 19/60\tloss: 0.292\tloss_act:0.292\n",
            "Batch: 22/60\tloss: 0.295\tloss_act:0.295\n",
            "Batch: 25/60\tloss: 0.267\tloss_act:0.267\n",
            "Batch: 28/60\tloss: 0.268\tloss_act:0.268\n",
            "Batch: 31/60\tloss: 0.241\tloss_act:0.241\n",
            "Batch: 34/60\tloss: 0.307\tloss_act:0.307\n",
            "Batch: 37/60\tloss: 0.229\tloss_act:0.229\n",
            "Batch: 40/60\tloss: 0.279\tloss_act:0.279\n",
            "Batch: 43/60\tloss: 0.291\tloss_act:0.291\n",
            "Batch: 46/60\tloss: 0.257\tloss_act:0.257\n",
            "Batch: 49/60\tloss: 0.264\tloss_act:0.264\n",
            "Batch: 52/60\tloss: 0.255\tloss_act:0.255\n",
            "Batch: 55/60\tloss: 0.265\tloss_act:0.265\n",
            "Batch: 58/60\tloss: 0.209\tloss_act:0.209\n",
            "Batch: 60/60\tloss: 0.245\tloss_act:0.245\n",
            "Epoch 8\tTrain Loss: 0.263\tVal Acc: 0.886\tTest Acc: 0.902\n",
            "Best Epoch: 8\tBest Epoch Val Acc: 0.886\tBest Epoch Test Acc: 0.902, Best Test Acc: 0.903\n",
            "\n",
            "********************Epoch: 9********************\n",
            "Batch: 1/60\tloss: 0.206\tloss_act:0.206\n",
            "Batch: 4/60\tloss: 0.283\tloss_act:0.283\n",
            "Batch: 7/60\tloss: 0.201\tloss_act:0.201\n",
            "Batch: 10/60\tloss: 0.287\tloss_act:0.287\n",
            "Batch: 13/60\tloss: 0.269\tloss_act:0.269\n",
            "Batch: 16/60\tloss: 0.255\tloss_act:0.255\n",
            "Batch: 19/60\tloss: 0.270\tloss_act:0.270\n",
            "Batch: 22/60\tloss: 0.305\tloss_act:0.305\n",
            "Batch: 25/60\tloss: 0.249\tloss_act:0.249\n",
            "Batch: 28/60\tloss: 0.309\tloss_act:0.309\n",
            "Batch: 31/60\tloss: 0.289\tloss_act:0.289\n",
            "Batch: 34/60\tloss: 0.253\tloss_act:0.253\n",
            "Batch: 37/60\tloss: 0.262\tloss_act:0.262\n",
            "Batch: 40/60\tloss: 0.239\tloss_act:0.239\n",
            "Batch: 43/60\tloss: 0.268\tloss_act:0.268\n",
            "Batch: 46/60\tloss: 0.318\tloss_act:0.318\n",
            "Batch: 49/60\tloss: 0.268\tloss_act:0.268\n",
            "Batch: 52/60\tloss: 0.258\tloss_act:0.258\n",
            "Batch: 55/60\tloss: 0.210\tloss_act:0.210\n",
            "Batch: 58/60\tloss: 0.306\tloss_act:0.306\n",
            "Batch: 60/60\tloss: 0.239\tloss_act:0.239\n",
            "Epoch 9\tTrain Loss: 0.261\tVal Acc: 0.884\tTest Acc: 0.902\n",
            "Best Epoch: 8\tBest Epoch Val Acc: 0.886\tBest Epoch Test Acc: 0.902, Best Test Acc: 0.903\n",
            "\n",
            "********************Epoch: 10********************\n",
            "Batch: 1/60\tloss: 0.249\tloss_act:0.249\n",
            "Batch: 4/60\tloss: 0.264\tloss_act:0.264\n",
            "Batch: 7/60\tloss: 0.236\tloss_act:0.236\n",
            "Batch: 10/60\tloss: 0.236\tloss_act:0.236\n",
            "Batch: 13/60\tloss: 0.242\tloss_act:0.242\n",
            "Batch: 16/60\tloss: 0.248\tloss_act:0.248\n",
            "Batch: 19/60\tloss: 0.284\tloss_act:0.284\n",
            "Batch: 22/60\tloss: 0.272\tloss_act:0.272\n",
            "Batch: 25/60\tloss: 0.232\tloss_act:0.232\n",
            "Batch: 28/60\tloss: 0.254\tloss_act:0.254\n",
            "Batch: 31/60\tloss: 0.246\tloss_act:0.246\n",
            "Batch: 34/60\tloss: 0.231\tloss_act:0.231\n",
            "Batch: 37/60\tloss: 0.233\tloss_act:0.233\n",
            "Batch: 40/60\tloss: 0.203\tloss_act:0.203\n",
            "Batch: 43/60\tloss: 0.250\tloss_act:0.250\n",
            "Batch: 46/60\tloss: 0.260\tloss_act:0.260\n",
            "Batch: 49/60\tloss: 0.223\tloss_act:0.223\n",
            "Batch: 52/60\tloss: 0.250\tloss_act:0.250\n",
            "Batch: 55/60\tloss: 0.313\tloss_act:0.313\n",
            "Batch: 58/60\tloss: 0.271\tloss_act:0.271\n",
            "Batch: 60/60\tloss: 0.208\tloss_act:0.208\n",
            "Epoch 10\tTrain Loss: 0.259\tVal Acc: 0.880\tTest Acc: 0.900\n",
            "Best Epoch: 8\tBest Epoch Val Acc: 0.886\tBest Epoch Test Acc: 0.902, Best Test Acc: 0.903\n",
            "\n",
            "********************Epoch: 11********************\n",
            "Batch: 1/60\tloss: 0.310\tloss_act:0.310\n",
            "Batch: 4/60\tloss: 0.246\tloss_act:0.246\n",
            "Batch: 7/60\tloss: 0.232\tloss_act:0.232\n",
            "Batch: 10/60\tloss: 0.217\tloss_act:0.217\n",
            "Batch: 13/60\tloss: 0.190\tloss_act:0.190\n",
            "Batch: 16/60\tloss: 0.341\tloss_act:0.341\n",
            "Batch: 19/60\tloss: 0.304\tloss_act:0.304\n",
            "Batch: 22/60\tloss: 0.261\tloss_act:0.261\n",
            "Batch: 25/60\tloss: 0.221\tloss_act:0.221\n",
            "Batch: 28/60\tloss: 0.234\tloss_act:0.234\n",
            "Batch: 31/60\tloss: 0.279\tloss_act:0.279\n",
            "Batch: 34/60\tloss: 0.283\tloss_act:0.283\n",
            "Batch: 37/60\tloss: 0.287\tloss_act:0.287\n",
            "Batch: 40/60\tloss: 0.285\tloss_act:0.285\n",
            "Batch: 43/60\tloss: 0.231\tloss_act:0.231\n",
            "Batch: 46/60\tloss: 0.232\tloss_act:0.232\n",
            "Batch: 49/60\tloss: 0.262\tloss_act:0.262\n",
            "Batch: 52/60\tloss: 0.232\tloss_act:0.232\n",
            "Batch: 55/60\tloss: 0.261\tloss_act:0.261\n",
            "Batch: 58/60\tloss: 0.304\tloss_act:0.304\n",
            "Batch: 60/60\tloss: 0.260\tloss_act:0.260\n",
            "Epoch 11\tTrain Loss: 0.256\tVal Acc: 0.889\tTest Acc: 0.906\n",
            "Best Epoch: 11\tBest Epoch Val Acc: 0.889\tBest Epoch Test Acc: 0.906, Best Test Acc: 0.906\n",
            "\n",
            "********************Epoch: 12********************\n",
            "Batch: 1/60\tloss: 0.247\tloss_act:0.247\n",
            "Batch: 4/60\tloss: 0.232\tloss_act:0.232\n",
            "Batch: 7/60\tloss: 0.253\tloss_act:0.253\n",
            "Batch: 10/60\tloss: 0.282\tloss_act:0.282\n",
            "Batch: 13/60\tloss: 0.258\tloss_act:0.258\n",
            "Batch: 16/60\tloss: 0.248\tloss_act:0.248\n",
            "Batch: 19/60\tloss: 0.245\tloss_act:0.245\n",
            "Batch: 22/60\tloss: 0.215\tloss_act:0.215\n",
            "Batch: 25/60\tloss: 0.277\tloss_act:0.277\n",
            "Batch: 28/60\tloss: 0.213\tloss_act:0.213\n",
            "Batch: 31/60\tloss: 0.274\tloss_act:0.274\n",
            "Batch: 34/60\tloss: 0.314\tloss_act:0.314\n",
            "Batch: 37/60\tloss: 0.219\tloss_act:0.219\n",
            "Batch: 40/60\tloss: 0.254\tloss_act:0.254\n",
            "Batch: 43/60\tloss: 0.263\tloss_act:0.263\n",
            "Batch: 46/60\tloss: 0.225\tloss_act:0.225\n",
            "Batch: 49/60\tloss: 0.222\tloss_act:0.222\n",
            "Batch: 52/60\tloss: 0.266\tloss_act:0.266\n",
            "Batch: 55/60\tloss: 0.240\tloss_act:0.240\n",
            "Batch: 58/60\tloss: 0.217\tloss_act:0.217\n",
            "Batch: 60/60\tloss: 0.202\tloss_act:0.202\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/speak-turn-emb-dialog-act-clf/engine.py\", line 193, in <module>\n",
            "    engine.train()\n",
            "  File \"/content/speak-turn-emb-dialog-act-clf/engine.py\", line 76, in train\n",
            "    acc = self.eval()\n",
            "          ^^^^^^^^^^^\n",
            "  File \"/content/speak-turn-emb-dialog-act-clf/engine.py\", line 135, in eval\n",
            "    outputs = self.model(input_ids, attention_mask, chunk_lens, speaker_ids, topic_labels)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\", line 191, in forward\n",
            "    return self.module(*inputs[0], **module_kwargs[0])\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/speak-turn-emb-dialog-act-clf/models.py\", line 58, in forward\n",
            "    embeddings = self.bert(batch[0], attention_mask=batch[1], output_hidden_states=True)[0][:, 0]\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 852, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "                      ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 527, in forward\n",
            "    layer_outputs = layer_module(\n",
            "                    ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 453, in forward\n",
            "    layer_output = apply_chunking_to_forward(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\", line 236, in apply_chunking_to_forward\n",
            "    return forward_fn(*input_tensors)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 466, in feed_forward_chunk\n",
            "    layer_output = self.output(intermediate_output, attention_output)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 377, in forward\n",
            "    hidden_states = self.dense(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python run_mrda.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s771fCd3rWR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af41802-3dbc-4489-eb05-cf46172c9c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python -u engine.py --corpus=dyda --mode=train --gpu=0,1 --batch_size=10 --batch_size_val=10 --epochs=100 --lr=0.0001 --nlayer=2 --chunk_size=0 --dropout=0.5 --nfinetune=1  --speaker_info=emb_cls --topic_info=emb_cls --nclass=4 --emb_batch=0\n",
            "Namespace(corpus='dyda', mode='train', nclass=4, batch_size=10, batch_size_val=10, emb_batch=0, epochs=100, gpu='0,1', lr=0.0001, nlayer=2, chunk_size=0, dropout=0.5, speaker_info='emb_cls', topic_info='emb_cls', nfinetune=1, seed=0)\n",
            "Tokenizing train....\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Done\n",
            "Tokenizing val....\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Done\n",
            "Tokenizing test....\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Done\n",
            "Done\n",
            "\n",
            "Let's use 1 GPUs!\n",
            "Initializing model....\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2025-05-07 17:01:36.904322: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746637296.922961    9501 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746637296.928357    9501 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "********************Epoch: 1********************\n",
            "Batch: 1/1112\tloss: 1.447\tloss_act:1.447\n",
            "Batch: 56/1112\tloss: 1.273\tloss_act:1.273\n",
            "Batch: 111/1112\tloss: 0.938\tloss_act:0.938\n",
            "Batch: 166/1112\tloss: 0.853\tloss_act:0.853\n",
            "Batch: 221/1112\tloss: 0.643\tloss_act:0.643\n",
            "Batch: 276/1112\tloss: 0.630\tloss_act:0.630\n",
            "Batch: 331/1112\tloss: 0.742\tloss_act:0.742\n",
            "Batch: 386/1112\tloss: 0.558\tloss_act:0.558\n",
            "Batch: 441/1112\tloss: 0.705\tloss_act:0.705\n",
            "Batch: 496/1112\tloss: 0.515\tloss_act:0.515\n",
            "Batch: 551/1112\tloss: 0.351\tloss_act:0.351\n",
            "Batch: 606/1112\tloss: 0.549\tloss_act:0.549\n",
            "Batch: 661/1112\tloss: 0.373\tloss_act:0.373\n",
            "Batch: 716/1112\tloss: 0.297\tloss_act:0.297\n",
            "Batch: 771/1112\tloss: 0.419\tloss_act:0.419\n",
            "Batch: 826/1112\tloss: 0.493\tloss_act:0.493\n",
            "Batch: 881/1112\tloss: 0.293\tloss_act:0.293\n",
            "Batch: 936/1112\tloss: 0.363\tloss_act:0.363\n",
            "Batch: 991/1112\tloss: 0.585\tloss_act:0.585\n",
            "Batch: 1046/1112\tloss: 0.332\tloss_act:0.332\n",
            "Batch: 1101/1112\tloss: 0.271\tloss_act:0.271\n",
            "Batch: 1112/1112\tloss: 0.377\tloss_act:0.377\n",
            "Epoch 1\tTrain Loss: 0.571\tVal Acc: 0.822\tTest Acc: 0.854\n",
            "Best Epoch: 1\tBest Epoch Val Acc: 0.822\tBest Epoch Test Acc: 0.854, Best Test Acc: 0.854\n",
            "\n",
            "********************Epoch: 2********************\n",
            "Batch: 1/1112\tloss: 0.319\tloss_act:0.319\n",
            "Batch: 56/1112\tloss: 0.432\tloss_act:0.432\n",
            "Batch: 111/1112\tloss: 0.493\tloss_act:0.493\n",
            "Batch: 166/1112\tloss: 0.365\tloss_act:0.365\n",
            "Batch: 221/1112\tloss: 0.442\tloss_act:0.442\n",
            "Batch: 276/1112\tloss: 0.375\tloss_act:0.375\n",
            "Batch: 331/1112\tloss: 0.607\tloss_act:0.607\n",
            "Batch: 386/1112\tloss: 0.219\tloss_act:0.219\n",
            "Batch: 441/1112\tloss: 0.338\tloss_act:0.338\n",
            "Batch: 496/1112\tloss: 0.299\tloss_act:0.299\n",
            "Batch: 551/1112\tloss: 0.472\tloss_act:0.472\n",
            "Batch: 606/1112\tloss: 0.607\tloss_act:0.607\n",
            "Batch: 661/1112\tloss: 0.343\tloss_act:0.343\n",
            "Batch: 716/1112\tloss: 0.342\tloss_act:0.342\n",
            "Batch: 771/1112\tloss: 0.267\tloss_act:0.267\n",
            "Batch: 826/1112\tloss: 0.306\tloss_act:0.306\n",
            "Batch: 881/1112\tloss: 0.414\tloss_act:0.414\n",
            "Batch: 936/1112\tloss: 0.277\tloss_act:0.277\n",
            "Batch: 991/1112\tloss: 0.385\tloss_act:0.385\n",
            "Batch: 1046/1112\tloss: 0.397\tloss_act:0.397\n",
            "Batch: 1101/1112\tloss: 0.319\tloss_act:0.319\n",
            "Batch: 1112/1112\tloss: 0.716\tloss_act:0.716\n",
            "Epoch 2\tTrain Loss: 0.411\tVal Acc: 0.836\tTest Acc: 0.859\n",
            "Best Epoch: 2\tBest Epoch Val Acc: 0.836\tBest Epoch Test Acc: 0.859, Best Test Acc: 0.859\n",
            "\n",
            "********************Epoch: 3********************\n",
            "Batch: 1/1112\tloss: 0.423\tloss_act:0.423\n",
            "Batch: 56/1112\tloss: 0.324\tloss_act:0.324\n",
            "Batch: 111/1112\tloss: 0.462\tloss_act:0.462\n",
            "Batch: 166/1112\tloss: 0.263\tloss_act:0.263\n",
            "Batch: 221/1112\tloss: 0.359\tloss_act:0.359\n",
            "Batch: 276/1112\tloss: 0.366\tloss_act:0.366\n",
            "Batch: 331/1112\tloss: 0.550\tloss_act:0.550\n",
            "Batch: 386/1112\tloss: 0.547\tloss_act:0.547\n",
            "Batch: 441/1112\tloss: 0.311\tloss_act:0.311\n",
            "Batch: 496/1112\tloss: 0.478\tloss_act:0.478\n",
            "Batch: 551/1112\tloss: 0.328\tloss_act:0.328\n",
            "Batch: 606/1112\tloss: 0.365\tloss_act:0.365\n",
            "Batch: 661/1112\tloss: 0.413\tloss_act:0.413\n",
            "Batch: 716/1112\tloss: 0.408\tloss_act:0.408\n",
            "Batch: 771/1112\tloss: 0.210\tloss_act:0.210\n",
            "Batch: 826/1112\tloss: 0.596\tloss_act:0.596\n",
            "Batch: 881/1112\tloss: 0.287\tloss_act:0.287\n",
            "Batch: 936/1112\tloss: 0.318\tloss_act:0.318\n",
            "Batch: 991/1112\tloss: 0.791\tloss_act:0.791\n",
            "Batch: 1046/1112\tloss: 0.312\tloss_act:0.312\n",
            "Batch: 1101/1112\tloss: 0.510\tloss_act:0.510\n",
            "Batch: 1112/1112\tloss: 0.729\tloss_act:0.729\n",
            "Epoch 3\tTrain Loss: 0.387\tVal Acc: 0.840\tTest Acc: 0.858\n",
            "Best Epoch: 3\tBest Epoch Val Acc: 0.840\tBest Epoch Test Acc: 0.858, Best Test Acc: 0.859\n",
            "\n",
            "********************Epoch: 4********************\n",
            "Batch: 1/1112\tloss: 0.218\tloss_act:0.218\n",
            "Batch: 56/1112\tloss: 0.296\tloss_act:0.296\n",
            "Batch: 111/1112\tloss: 0.280\tloss_act:0.280\n",
            "Batch: 166/1112\tloss: 0.259\tloss_act:0.259\n",
            "Batch: 221/1112\tloss: 0.620\tloss_act:0.620\n",
            "Batch: 276/1112\tloss: 0.203\tloss_act:0.203\n",
            "Batch: 331/1112\tloss: 0.370\tloss_act:0.370\n",
            "Batch: 386/1112\tloss: 0.320\tloss_act:0.320\n",
            "Batch: 441/1112\tloss: 0.400\tloss_act:0.400\n",
            "Batch: 496/1112\tloss: 0.457\tloss_act:0.457\n",
            "Batch: 551/1112\tloss: 0.364\tloss_act:0.364\n",
            "Batch: 606/1112\tloss: 0.437\tloss_act:0.437\n",
            "Batch: 661/1112\tloss: 0.268\tloss_act:0.268\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/speak-turn-emb-dialog-act-clf/engine.py\", line 193, in <module>\n",
            "    engine.train()\n",
            "  File \"/content/speak-turn-emb-dialog-act-clf/engine.py\", line 75, in train\n",
            "    loss = self.train_epoch()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/speak-turn-emb-dialog-act-clf/engine.py\", line 108, in train_epoch\n",
            "    outputs = self.model(input_ids, attention_mask, chunk_lens, speaker_ids,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\", line 191, in forward\n",
            "    return self.module(*inputs[0], **module_kwargs[0])\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/speak-turn-emb-dialog-act-clf/models.py\", line 76, in forward\n",
            "    embeddings = pack_padded_sequence(embeddings, chunk_lens, enforce_sorted=False)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/utils/rnn.py\", line 337, in pack_padded_sequence\n",
            "    sorted_indices = sorted_indices.to(input.device)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python run_dyda.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}